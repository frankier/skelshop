{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is this? # This is the documentation for SkelShop . See also the README for a bit of extra background. Feel free to ask on GitHub discussions for help. Getting started # There are 2 options: A Docker container: Which is run with the Docker runtime in general usage with a personal computers/workstations. And run with Singularity in HPC environments. Manual setup, which may be convenient for development of SkelShop, usage as a library, or if you want to use GUI components such as the playsticks command. (It may also be possible to run playsticks with x11docker . Please let me know if you get this working so I can add it to these instructions.) You may find it easiest to combine these, e.g. dump skeletons from one of the container, while only running playsticks with the manual installation. Running the Docker container with Docker # There are two Docker containers, one based on CUDA and one able to run (slowly...) using only a CPU. For the CUDA (10.2) version, make sure you have CUDA 10 and nvidia-docker on the host and then run: $ docker run --nv frankierr/skelshop:focal_nvcaffe python -m skelshop --help For the CPU version: $ docker run frankierr/skelshop:focal_cpu python -m skelshop --help For more information about the Docker containers see their page on Docker Hub and the openpose_containers repository where the bases are built . Running the Docker container with Singularity # You can also run the container with Singularity, which is convenient in HPC environments. $ singularity pull skelshop.sif docker://frankierr/skelshop:focal_nvcaffe $ singularity run --nv skelshop.sif python -m skelshop --help Manual setup # First you need to install Poetry and OpenPose v1.7.0 . Then you can install using the script: $ git clone https://github.com/frankier/skelshop $ cd skelshop $ ./install_all.sh $ poetry run snakemake If you only want to run the playsticks command you do not need to install OpenPose, and can instead run: $ ./install.sh -E playsticks","title":"Getting started"},{"location":"#what-is-this","text":"This is the documentation for SkelShop . See also the README for a bit of extra background. Feel free to ask on GitHub discussions for help.","title":"What is this?"},{"location":"#getting-started","text":"There are 2 options: A Docker container: Which is run with the Docker runtime in general usage with a personal computers/workstations. And run with Singularity in HPC environments. Manual setup, which may be convenient for development of SkelShop, usage as a library, or if you want to use GUI components such as the playsticks command. (It may also be possible to run playsticks with x11docker . Please let me know if you get this working so I can add it to these instructions.) You may find it easiest to combine these, e.g. dump skeletons from one of the container, while only running playsticks with the manual installation.","title":"Getting started"},{"location":"#running-the-docker-container-with-docker","text":"There are two Docker containers, one based on CUDA and one able to run (slowly...) using only a CPU. For the CUDA (10.2) version, make sure you have CUDA 10 and nvidia-docker on the host and then run: $ docker run --nv frankierr/skelshop:focal_nvcaffe python -m skelshop --help For the CPU version: $ docker run frankierr/skelshop:focal_cpu python -m skelshop --help For more information about the Docker containers see their page on Docker Hub and the openpose_containers repository where the bases are built .","title":"Running the Docker container with Docker"},{"location":"#running-the-docker-container-with-singularity","text":"You can also run the container with Singularity, which is convenient in HPC environments. $ singularity pull skelshop.sif docker://frankierr/skelshop:focal_nvcaffe $ singularity run --nv skelshop.sif python -m skelshop --help","title":"Running the Docker container with Singularity"},{"location":"#manual-setup","text":"First you need to install Poetry and OpenPose v1.7.0 . Then you can install using the script: $ git clone https://github.com/frankier/skelshop $ cd skelshop $ ./install_all.sh $ poetry run snakemake If you only want to run the playsticks command you do not need to install OpenPose, and can instead run: $ ./install.sh -E playsticks","title":"Manual setup"},{"location":"benchmark/","text":"Benchmarks # Some benchmarks are given here, mainly to help with back of the envelope -type calculations of what level of resources might be needed for your own pipeline build from these stages on your own video corpus. The following benchmarks were run on the following machine: 7 cores of Intel Xeon Gold 5120 CPU @ 2.20GHz 1 P100 with 16GB RAM 32 GB RAM They were run on the 480p version of a typical talking heads video on YouTube . The video is just over 2 minutes, so use this to get an approximate scaling factor for your own corpus. All times include the script/Singularity startup time, so this should be taken into account. System Time (m:s) Shot Segmentors PySceneDetect 10.4 ffprobe 10.7 Pose estimators OpenPose Body25 4:23.8 OpenPose Body25+Face 8:53.9 OpenPose Body25+Hands 10:11.2 OpenPose Body25+Face+Hands 14:49.7 Within-shot pose trackers (run on output of Body25) lighttrackish 33.5 opt_lighttrack 17.7 deepsortlike 21.8 Face detection + embedding (this based upon embedding all faces found) dlib-hog-face5 2:50.1 dlib-hog-face68 2:53.1 dlib-cnn-face5 0:44.5 dlib-cnn-face68 0:43.7 Embedding faces from existing OpenPose (face3 only requires Body25, whereas face68 requires Body25+Face) openpose-face68 17.3 openpose-face3 21.0 Accuracy comparisons # In many cases, accuracy comparisons can be found on the original websites or papers of the software providing the different pipeline stages. However, here are some extra comparisons available elsewhere. Note that these may refer to old versions: Dlib on LFW Shot detection benchmarks Master's thesis analysing failure modes of different face embeddings including dlib's","title":"Benchmarks"},{"location":"benchmark/#benchmarks","text":"Some benchmarks are given here, mainly to help with back of the envelope -type calculations of what level of resources might be needed for your own pipeline build from these stages on your own video corpus. The following benchmarks were run on the following machine: 7 cores of Intel Xeon Gold 5120 CPU @ 2.20GHz 1 P100 with 16GB RAM 32 GB RAM They were run on the 480p version of a typical talking heads video on YouTube . The video is just over 2 minutes, so use this to get an approximate scaling factor for your own corpus. All times include the script/Singularity startup time, so this should be taken into account. System Time (m:s) Shot Segmentors PySceneDetect 10.4 ffprobe 10.7 Pose estimators OpenPose Body25 4:23.8 OpenPose Body25+Face 8:53.9 OpenPose Body25+Hands 10:11.2 OpenPose Body25+Face+Hands 14:49.7 Within-shot pose trackers (run on output of Body25) lighttrackish 33.5 opt_lighttrack 17.7 deepsortlike 21.8 Face detection + embedding (this based upon embedding all faces found) dlib-hog-face5 2:50.1 dlib-hog-face68 2:53.1 dlib-cnn-face5 0:44.5 dlib-cnn-face68 0:43.7 Embedding faces from existing OpenPose (face3 only requires Body25, whereas face68 requires Body25+Face) openpose-face68 17.3 openpose-face3 21.0","title":"Benchmarks"},{"location":"benchmark/#accuracy-comparisons","text":"In many cases, accuracy comparisons can be found on the original websites or papers of the software providing the different pipeline stages. However, here are some extra comparisons available elsewhere. Note that these may refer to old versions: Dlib on LFW Shot detection benchmarks Master's thesis analysing failure modes of different face embeddings including dlib's","title":"Accuracy comparisons"},{"location":"cli/","text":"CLI Reference # skelshop # Usage: skelshop [OPTIONS] COMMAND [ARGS]... Options: -v, --verbosity LVL Either CRITICAL, ERROR, WARNING, INFO or DEBUG --ffprobe-bin PATH If you cannot install ffprobe globally, you can provide the path to the version you want to use here bench # Commands to benchmark SkelShop's I/O speeds. Usage: skelshop bench [OPTIONS] COMMAND [ARGS]... read-shot-seg # Benchmark reading a shot segmented skeleton file. Usage: skelshop bench read-shot-seg [OPTIONS] SKELS_FN write-bench-script # Usage: skelshop bench write-bench-script [OPTIONS] HEADER YOUTUBE_ID OUT_FN Options: --sif TEXT calibrate # Keypoint calibration tools. Usage: skelshop calibrate [OPTIONS] COMMAND [ARGS]... analyse # Usage: skelshop calibrate analyse [OPTIONS] DFIN Options: --chart-out PATH --kps TEXT --thresh FLOAT --excl-thresh FLOAT --mask TEXT process-dlib-dir # Give a directory with dlib facepoint XMLs, run OpenPose on all images and write out where the keypoints are relative to the chips. Usage: skelshop calibrate process-dlib-dir [OPTIONS] DIRIN H5IN DFOUT Options: --add-symmetries / --no-add-symmetries --add-synthetic / --no-add-synthetic process-video # Given a video and a skeleton keypoint file, run the dlib keypoint detection pipeline for each frame and give points where BODY_25 face keypoints would have to map to make the same transformation. Usage: skelshop calibrate process-video [OPTIONS] VIDEO H5INFN DFOUT conv # Convert a exiting dump from another format into HDF5 format. LEGACY_DUMP is the dump in the old format. OUT is a file path when run with single-zip, otherwise it is the base of a directory tree which will be created during processing. Usage: skelshop conv [OPTIONS] [monolithic-tar|single-zip|ordered-tar] LEGACY_DUMP [OUT] Options: --mode [BODY_25_ALL|BODY_25_HANDS|BODY_25|BODY_135|FACE|BODY_25_FACE] [required] --cores INTEGER Number of cores to use (only for monolithic- tar) --suppress-end-fail / --no-suppress-end-fail --skip-existing / --overwrite-existing drawsticks # Output a video with stick figures from pose dump superimposed. Usage: skelshop drawsticks [OPTIONS] H5FN VIDEOIN VIDEOOUT Options: --posetrack / --no-posetrack Whether to convert BODY_25 keypoints to PoseTrack-style keypoints --scale INTEGER --overlay / --no-overlay Whether to draw VIDEOIN below the stick figures or not dump # Create a HDF5 pose dump from a video using OpenPose. This command optionally applies steps from the tracking/segmentation pipeline. Usage: skelshop dump [OPTIONS] VIDEO H5FN Options: --compression [none|lossless|lossless9|lossy|lossy9] --mode [BODY_25_ALL|BODY_25_HANDS|BODY_25|BODY_135|FACE|BODY_25_FACE] --model-folder TEXT [required] --debug / --no-debug --dry-run / --write --segs-file PATH --pose-matcher-config TEXT --track-conf [lighttrackish|opt_lighttrack|deepsortlike] --track / --no-track --shot-seg [bbskel|psd|ffprobe|none] dumpimgs # Dump a directory of images to a HDF5 file using OpenPose. Usage: skelshop dumpimgs [OPTIONS] INPUT_DIR H5OUT Options: --mode [BODY_25_ALL|BODY_25_HANDS|BODY_25|BODY_135|FACE|BODY_25_FACE] --model-folder TEXT [required] face # Commands for creating face embedding dumps Usage: skelshop face [OPTIONS] COMMAND [ARGS]... bestcands # Select the best frame-person pairs to use for face embedding. Usage: skelshop face bestcands [OPTIONS] [conf-face3|conf-face5|conf-face68|conf-adaptive|clear-face3|clear- face5|clear-face68|clear-adaptive] SKELIN SEGSOUT Options: --video PATH embedall # Create a HDF5 face dump from a video using dlib. Usage: skelshop face embedall [OPTIONS] [dlib-hog-face5|dlib-hog-face68|dlib-cnn-face5|dlib-cnn- face68|openpose-face68|openpose-face3|openpose-face5] VIDEO H5FN Options: --from-skels PATH --start-frame INTEGER --skel-thresh-pool [min|max|mean] --skel-thresh-val FLOAT --batch-size INTEGER --write-bboxes / --no-write-bboxes --write-chip / --no-write-chip embedselect # Embed faces into a sparse face dump according to a predetermined selection of frame-person pairs. Usage: skelshop face embedselect [OPTIONS] VIDEO SELECTION H5FN Options: --from-skels PATH --batch-size TEXT The batch size to use. Valid values are 'guess' or an integer. The default is to guess on GPU otherwise use 16 --write-bboxes / --no-write-bboxes --write-chip / --no-write-chip savechips # Extract chips into image files from a face dump with chips embedded inside it. Usage: skelshop face savechips [OPTIONS] H5FIN GROUPIN IMGOUT Options: --group-fmt [psd|ffprobe|trackshot] filter # Apply tracking to an untracked HDF5 pose dump. Usage: skelshop filter [OPTIONS] H5INFN H5OUTFN Options: --segs-file PATH --pose-matcher-config TEXT --track-conf [lighttrackish|opt_lighttrack|deepsortlike] --track / --no-track --shot-seg [bbskel|psd|ffprobe|none] --compression [none|lossless|lossless9|lossy|lossy9] --start-frame INTEGER --end-frame INTEGER iden # Commands to do with identification based on face embedding dumps Usage: skelshop iden [OPTIONS] COMMAND [ARGS]... applymap # Apply a mapping from clusters to known IDs (e.g. Wikidata Q-ids) Usage: skelshop iden applymap [OPTIONS] SEGS_IN ASSIGN_IN SEGS_OUT Options: --filter-unlabeled / --keep-unlabeled buildrefs # Build a reference face library from a list of entities or categories. Usage: skelshop iden buildrefs [OPTIONS] [entities|categories] LISTIN REFOUT [[wikidata|commons]] Options: --oauth-creds FILENAME clus # Clusters embeddings from multiple videos descriped in a corpus description file. Usage: skelshop iden clus [OPTIONS] COMMAND [ARGS]... fixed # Performs dbscan with fixed parameters. Usage: skelshop iden clus fixed [OPTIONS] CORPUS_DESC Options: --n-jobs INTEGER --sample-size INTEGER --knn INTEGER --pool [med|min|vote] --ann-lib [pynndescent|faiss-exact] --algorithm [dbscan|optics-dbscan|rnn-dbscan] --num-protos INTEGER --model-out PATH --proto-out PATH --corpus-base PATH --eps FLOAT --min-samples INTEGER search # Performs grid search to find best clustering parameters. Usage: skelshop iden clus search [OPTIONS] CORPUS_DESC Options: --n-jobs INTEGER --sample-size INTEGER --knn INTEGER --pool [med|min|vote] --ann-lib [pynndescent|faiss-exact] --algorithm [dbscan|optics-dbscan|rnn-dbscan] --num-protos INTEGER --model-out PATH --proto-out PATH --corpus-base PATH --eps TEXT --min-samples TEXT --score [both|silhouette|tracks-acc] embedrefs # Pre-embed a face reference directory structure into a HDF5 file. Usage: skelshop iden embedrefs [OPTIONS] INPUT_DIR H5OUT idclus # Identifies clusters by comparing against a reference and forcing a match Usage: skelshop iden idclus [OPTIONS] REF_IN PROTOS CORPUS_DESC ASSIGN_OUT Options: --thresh FLOAT --corpus-base PATH idrnnclus # Identifies clusters by comparing against a reference and forcing a match Usage: skelshop iden idrnnclus [OPTIONS] REF_IN MODELIN ASSIGN_OUT Options: --thresh FLOAT idsegsfull # Identifies shots with a particular person from reference headshots using face dumps which contain all face embeddings in a clip. Usage: skelshop iden idsegsfull [OPTIONS] REF_IN GROUPIN FACES SEGSOUT Options: --group-fmt [psd|ffprobe|trackshot] --min-detected-frames INTEGER --detection-threshold FLOAT --median-threshold FLOAT idsegssparse # Identifies shots with a particular person from reference headshots using face dumps with only a few (fixed number) embeddings per person-shot pair. Usage: skelshop iden idsegssparse [OPTIONS] REF_IN GROUPIN FACES SEGSOUT Options: --group-fmt [psd|ffprobe|trackshot] --detection-threshold FLOAT whoisthis # Given a directory of prototype images as generated by writeprotos , produces a cluster identification mapping by interactively asking the you to identify people. Usage: skelshop iden whoisthis [OPTIONS] PROTOS_DIR ASSIGN_OUT Options: --thumbview-cmd TEXT writeprotos # Given prototypes description file as produced by clus , dumps the corresponding images. Usage: skelshop iden writeprotos [OPTIONS] PROTOS CORPUS_DESC PROTOS_DIR Options: --corpus-base PATH playsticks # Play a video with stick figures from pose dump superimposed. Usage: skelshop playsticks [OPTIONS] VIDEOIN Options: --skel PATH --face PATH --posetrack / --no-posetrack Whether to convert BODY_25 keypoints to PoseTrack-style keypoints --seek-time FLOAT --seek-frame INTEGER --scale INTEGER --paused / --playing stats # Output stats about dumps in INPUT_DIR. Usage: skelshop stats [OPTIONS] INPUT_DIR","title":"CLI reference"},{"location":"cli/#cli-reference","text":"","title":"CLI Reference"},{"location":"cli/#skelshop","text":"Usage: skelshop [OPTIONS] COMMAND [ARGS]... Options: -v, --verbosity LVL Either CRITICAL, ERROR, WARNING, INFO or DEBUG --ffprobe-bin PATH If you cannot install ffprobe globally, you can provide the path to the version you want to use here","title":"skelshop"},{"location":"cli/#bench","text":"Commands to benchmark SkelShop's I/O speeds. Usage: skelshop bench [OPTIONS] COMMAND [ARGS]...","title":"bench"},{"location":"cli/#read-shot-seg","text":"Benchmark reading a shot segmented skeleton file. Usage: skelshop bench read-shot-seg [OPTIONS] SKELS_FN","title":"read-shot-seg"},{"location":"cli/#write-bench-script","text":"Usage: skelshop bench write-bench-script [OPTIONS] HEADER YOUTUBE_ID OUT_FN Options: --sif TEXT","title":"write-bench-script"},{"location":"cli/#calibrate","text":"Keypoint calibration tools. Usage: skelshop calibrate [OPTIONS] COMMAND [ARGS]...","title":"calibrate"},{"location":"cli/#analyse","text":"Usage: skelshop calibrate analyse [OPTIONS] DFIN Options: --chart-out PATH --kps TEXT --thresh FLOAT --excl-thresh FLOAT --mask TEXT","title":"analyse"},{"location":"cli/#process-dlib-dir","text":"Give a directory with dlib facepoint XMLs, run OpenPose on all images and write out where the keypoints are relative to the chips. Usage: skelshop calibrate process-dlib-dir [OPTIONS] DIRIN H5IN DFOUT Options: --add-symmetries / --no-add-symmetries --add-synthetic / --no-add-synthetic","title":"process-dlib-dir"},{"location":"cli/#process-video","text":"Given a video and a skeleton keypoint file, run the dlib keypoint detection pipeline for each frame and give points where BODY_25 face keypoints would have to map to make the same transformation. Usage: skelshop calibrate process-video [OPTIONS] VIDEO H5INFN DFOUT","title":"process-video"},{"location":"cli/#conv","text":"Convert a exiting dump from another format into HDF5 format. LEGACY_DUMP is the dump in the old format. OUT is a file path when run with single-zip, otherwise it is the base of a directory tree which will be created during processing. Usage: skelshop conv [OPTIONS] [monolithic-tar|single-zip|ordered-tar] LEGACY_DUMP [OUT] Options: --mode [BODY_25_ALL|BODY_25_HANDS|BODY_25|BODY_135|FACE|BODY_25_FACE] [required] --cores INTEGER Number of cores to use (only for monolithic- tar) --suppress-end-fail / --no-suppress-end-fail --skip-existing / --overwrite-existing","title":"conv"},{"location":"cli/#drawsticks","text":"Output a video with stick figures from pose dump superimposed. Usage: skelshop drawsticks [OPTIONS] H5FN VIDEOIN VIDEOOUT Options: --posetrack / --no-posetrack Whether to convert BODY_25 keypoints to PoseTrack-style keypoints --scale INTEGER --overlay / --no-overlay Whether to draw VIDEOIN below the stick figures or not","title":"drawsticks"},{"location":"cli/#dump","text":"Create a HDF5 pose dump from a video using OpenPose. This command optionally applies steps from the tracking/segmentation pipeline. Usage: skelshop dump [OPTIONS] VIDEO H5FN Options: --compression [none|lossless|lossless9|lossy|lossy9] --mode [BODY_25_ALL|BODY_25_HANDS|BODY_25|BODY_135|FACE|BODY_25_FACE] --model-folder TEXT [required] --debug / --no-debug --dry-run / --write --segs-file PATH --pose-matcher-config TEXT --track-conf [lighttrackish|opt_lighttrack|deepsortlike] --track / --no-track --shot-seg [bbskel|psd|ffprobe|none]","title":"dump"},{"location":"cli/#dumpimgs","text":"Dump a directory of images to a HDF5 file using OpenPose. Usage: skelshop dumpimgs [OPTIONS] INPUT_DIR H5OUT Options: --mode [BODY_25_ALL|BODY_25_HANDS|BODY_25|BODY_135|FACE|BODY_25_FACE] --model-folder TEXT [required]","title":"dumpimgs"},{"location":"cli/#face","text":"Commands for creating face embedding dumps Usage: skelshop face [OPTIONS] COMMAND [ARGS]...","title":"face"},{"location":"cli/#bestcands","text":"Select the best frame-person pairs to use for face embedding. Usage: skelshop face bestcands [OPTIONS] [conf-face3|conf-face5|conf-face68|conf-adaptive|clear-face3|clear- face5|clear-face68|clear-adaptive] SKELIN SEGSOUT Options: --video PATH","title":"bestcands"},{"location":"cli/#embedall","text":"Create a HDF5 face dump from a video using dlib. Usage: skelshop face embedall [OPTIONS] [dlib-hog-face5|dlib-hog-face68|dlib-cnn-face5|dlib-cnn- face68|openpose-face68|openpose-face3|openpose-face5] VIDEO H5FN Options: --from-skels PATH --start-frame INTEGER --skel-thresh-pool [min|max|mean] --skel-thresh-val FLOAT --batch-size INTEGER --write-bboxes / --no-write-bboxes --write-chip / --no-write-chip","title":"embedall"},{"location":"cli/#embedselect","text":"Embed faces into a sparse face dump according to a predetermined selection of frame-person pairs. Usage: skelshop face embedselect [OPTIONS] VIDEO SELECTION H5FN Options: --from-skels PATH --batch-size TEXT The batch size to use. Valid values are 'guess' or an integer. The default is to guess on GPU otherwise use 16 --write-bboxes / --no-write-bboxes --write-chip / --no-write-chip","title":"embedselect"},{"location":"cli/#savechips","text":"Extract chips into image files from a face dump with chips embedded inside it. Usage: skelshop face savechips [OPTIONS] H5FIN GROUPIN IMGOUT Options: --group-fmt [psd|ffprobe|trackshot]","title":"savechips"},{"location":"cli/#filter","text":"Apply tracking to an untracked HDF5 pose dump. Usage: skelshop filter [OPTIONS] H5INFN H5OUTFN Options: --segs-file PATH --pose-matcher-config TEXT --track-conf [lighttrackish|opt_lighttrack|deepsortlike] --track / --no-track --shot-seg [bbskel|psd|ffprobe|none] --compression [none|lossless|lossless9|lossy|lossy9] --start-frame INTEGER --end-frame INTEGER","title":"filter"},{"location":"cli/#iden","text":"Commands to do with identification based on face embedding dumps Usage: skelshop iden [OPTIONS] COMMAND [ARGS]...","title":"iden"},{"location":"cli/#applymap","text":"Apply a mapping from clusters to known IDs (e.g. Wikidata Q-ids) Usage: skelshop iden applymap [OPTIONS] SEGS_IN ASSIGN_IN SEGS_OUT Options: --filter-unlabeled / --keep-unlabeled","title":"applymap"},{"location":"cli/#buildrefs","text":"Build a reference face library from a list of entities or categories. Usage: skelshop iden buildrefs [OPTIONS] [entities|categories] LISTIN REFOUT [[wikidata|commons]] Options: --oauth-creds FILENAME","title":"buildrefs"},{"location":"cli/#clus","text":"Clusters embeddings from multiple videos descriped in a corpus description file. Usage: skelshop iden clus [OPTIONS] COMMAND [ARGS]...","title":"clus"},{"location":"cli/#fixed","text":"Performs dbscan with fixed parameters. Usage: skelshop iden clus fixed [OPTIONS] CORPUS_DESC Options: --n-jobs INTEGER --sample-size INTEGER --knn INTEGER --pool [med|min|vote] --ann-lib [pynndescent|faiss-exact] --algorithm [dbscan|optics-dbscan|rnn-dbscan] --num-protos INTEGER --model-out PATH --proto-out PATH --corpus-base PATH --eps FLOAT --min-samples INTEGER","title":"fixed"},{"location":"cli/#search","text":"Performs grid search to find best clustering parameters. Usage: skelshop iden clus search [OPTIONS] CORPUS_DESC Options: --n-jobs INTEGER --sample-size INTEGER --knn INTEGER --pool [med|min|vote] --ann-lib [pynndescent|faiss-exact] --algorithm [dbscan|optics-dbscan|rnn-dbscan] --num-protos INTEGER --model-out PATH --proto-out PATH --corpus-base PATH --eps TEXT --min-samples TEXT --score [both|silhouette|tracks-acc]","title":"search"},{"location":"cli/#embedrefs","text":"Pre-embed a face reference directory structure into a HDF5 file. Usage: skelshop iden embedrefs [OPTIONS] INPUT_DIR H5OUT","title":"embedrefs"},{"location":"cli/#idclus","text":"Identifies clusters by comparing against a reference and forcing a match Usage: skelshop iden idclus [OPTIONS] REF_IN PROTOS CORPUS_DESC ASSIGN_OUT Options: --thresh FLOAT --corpus-base PATH","title":"idclus"},{"location":"cli/#idrnnclus","text":"Identifies clusters by comparing against a reference and forcing a match Usage: skelshop iden idrnnclus [OPTIONS] REF_IN MODELIN ASSIGN_OUT Options: --thresh FLOAT","title":"idrnnclus"},{"location":"cli/#idsegsfull","text":"Identifies shots with a particular person from reference headshots using face dumps which contain all face embeddings in a clip. Usage: skelshop iden idsegsfull [OPTIONS] REF_IN GROUPIN FACES SEGSOUT Options: --group-fmt [psd|ffprobe|trackshot] --min-detected-frames INTEGER --detection-threshold FLOAT --median-threshold FLOAT","title":"idsegsfull"},{"location":"cli/#idsegssparse","text":"Identifies shots with a particular person from reference headshots using face dumps with only a few (fixed number) embeddings per person-shot pair. Usage: skelshop iden idsegssparse [OPTIONS] REF_IN GROUPIN FACES SEGSOUT Options: --group-fmt [psd|ffprobe|trackshot] --detection-threshold FLOAT","title":"idsegssparse"},{"location":"cli/#whoisthis","text":"Given a directory of prototype images as generated by writeprotos , produces a cluster identification mapping by interactively asking the you to identify people. Usage: skelshop iden whoisthis [OPTIONS] PROTOS_DIR ASSIGN_OUT Options: --thumbview-cmd TEXT","title":"whoisthis"},{"location":"cli/#writeprotos","text":"Given prototypes description file as produced by clus , dumps the corresponding images. Usage: skelshop iden writeprotos [OPTIONS] PROTOS CORPUS_DESC PROTOS_DIR Options: --corpus-base PATH","title":"writeprotos"},{"location":"cli/#playsticks","text":"Play a video with stick figures from pose dump superimposed. Usage: skelshop playsticks [OPTIONS] VIDEOIN Options: --skel PATH --face PATH --posetrack / --no-posetrack Whether to convert BODY_25 keypoints to PoseTrack-style keypoints --seek-time FLOAT --seek-frame INTEGER --scale INTEGER --paused / --playing","title":"playsticks"},{"location":"cli/#stats","text":"Output stats about dumps in INPUT_DIR. Usage: skelshop stats [OPTIONS] INPUT_DIR","title":"stats"},{"location":"development/","text":"Please install the pre-commit based git hooks to run black and some basic code checks before submitting a PR. For example: $ pip install --user pre-commit && pre-commit install You can also run them manually at any time: $ ./run-checks.sh In case you make changes, but do not have OpenPose installed, don't forget that you can easily test your changes using Docker, e.g. $ DOCKER_TAG=focal_cpu DOCKERFILE_PATH=Dockerfile IMAGE_NAME=focal_cpu ./hooks/build $ docker run --mount type=bind,source=/,target=/host/ focal_cpu:latest python -m skelshop calibrate process-dlib-dir /host/to/dlib/examples/faces/ /host/to/skelshop/calib.dlib.pqt","title":"Contributing"},{"location":"face-embedders/","text":"Currently only dlib's face embedding model is supported. This uses Euclidean distance as a distance metric. Some information about is is available in this blog post.","title":"Face embedders"},{"location":"face-extractors/","text":"Before performing the face embedding step required for facial recognition, we need to extract rotated, non-aspect ratio preserving scaled images of an exact square size, since this is the type of data the embedders are trained on. There are currently two major approaches to extracting face chips : those based on dlib's pipeline and those based on OpenPose skeletons. Typically the latter is recommended if you are planning on extracting these skeleton keypoints at any point, since it is faster and allows the faces to be associated with tracked skeletons. dlib # The dlib based face chip extractor first runs a face detector and then as show in the pipelines overview The following modes are available: Mode name Face detector Face keypoint model dlib-hog-face5 HOG 5-point dlib-hog-face68 HOG 68-point dlib-cnn-face5 CNN 5-point dlib-cnn-face68 CNN 68-point The CNN detector is slower than the HOG detector, but can be GPU accelerated. The 5 point face keypoint model is slightly faster than the 68 keypoint model. Both keypoint models run on the CPU only. OpenPose # The OpenPose based face chip extractor uses keypoints detected by OpenPose. If your keypoint model (see meet the pose estimators for a list of keypoint models includes the face, you can use the 68 keypoint model, otherwise you can use the 3 keypoint model which uses only the face keypoints from BODY_25. Mode name Face keypoint model openpose-face3 3-point openpose-face68 68-point","title":"Face chip detectors/croppers"},{"location":"face-extractors/#dlib","text":"The dlib based face chip extractor first runs a face detector and then as show in the pipelines overview The following modes are available: Mode name Face detector Face keypoint model dlib-hog-face5 HOG 5-point dlib-hog-face68 HOG 68-point dlib-cnn-face5 CNN 5-point dlib-cnn-face68 CNN 68-point The CNN detector is slower than the HOG detector, but can be GPU accelerated. The 5 point face keypoint model is slightly faster than the 68 keypoint model. Both keypoint models run on the CPU only.","title":"dlib"},{"location":"face-extractors/#openpose","text":"The OpenPose based face chip extractor uses keypoints detected by OpenPose. If your keypoint model (see meet the pose estimators for a list of keypoint models includes the face, you can use the 68 keypoint model, otherwise you can use the 3 keypoint model which uses only the face keypoints from BODY_25. Mode name Face keypoint model openpose-face3 3-point openpose-face68 68-point","title":"OpenPose"},{"location":"face-pipeline-configuration-guide/","text":"When generating face embeddings, there are a number of choices to make: Decision Options Detection/localisation Dlib/OpenPose Which frames to embed faces from All/Only the best What to use for detection/localisation? # In almost all cases, OpenPose based detection and localisation should be used. The reason is twofold: Skeletons are currently the only entity which can be tracked in OpenPose. Associating face embeddings with a skeleton means they remain attached assigned to the correct person within each shot. Assuming you need to estimate a skeleton for other reasons, using these already estimated keypoints is faster. Embed all frames or only the best? # This question is not as clear, given that given a reasonable GPU, embedding faces is quite fast (at least for dlib), and so the benefit of only embedding a few keyframes is not as marked as might be expected. The reason for this is probably that the face embedding itself is fast enough that is comparable to the video decoding cost, which is necessary in both cases. So the choice of which to use is more a matter of what the downstream task needs. In general, for face embedding clustering, more points per shot-person is not necessarily a good thing and so embedding only the best is usually the best option (also referred to as producing a sparse face embedding dump). The reason is that the clustering algorithms have memory usage proportional. For direct identification, it's up to you, and it certainly may be the case that more accurate identification is possible from a full face embedding dump. Embedding all frames is done with the skelshop face embedall command, while embedding only the best is done with skelshop face bestcands followed by skelshop face embedselect .","title":"Guide: configuring the face embedding pipeline"},{"location":"face-pipeline-configuration-guide/#what-to-use-for-detectionlocalisation","text":"In almost all cases, OpenPose based detection and localisation should be used. The reason is twofold: Skeletons are currently the only entity which can be tracked in OpenPose. Associating face embeddings with a skeleton means they remain attached assigned to the correct person within each shot. Assuming you need to estimate a skeleton for other reasons, using these already estimated keypoints is faster.","title":"What to use for detection/localisation?"},{"location":"face-pipeline-configuration-guide/#embed-all-frames-or-only-the-best","text":"This question is not as clear, given that given a reasonable GPU, embedding faces is quite fast (at least for dlib), and so the benefit of only embedding a few keyframes is not as marked as might be expected. The reason for this is probably that the face embedding itself is fast enough that is comparable to the video decoding cost, which is necessary in both cases. So the choice of which to use is more a matter of what the downstream task needs. In general, for face embedding clustering, more points per shot-person is not necessarily a good thing and so embedding only the best is usually the best option (also referred to as producing a sparse face embedding dump). The reason is that the clustering algorithms have memory usage proportional. For direct identification, it's up to you, and it certainly may be the case that more accurate identification is possible from a full face embedding dump. Embedding all frames is done with the skelshop face embedall command, while embedding only the best is done with skelshop face bestcands followed by skelshop face embedselect .","title":"Embed all frames or only the best?"},{"location":"formats/","text":"The dump format is a HDF5 file: / - Contains metadata attributes such as: fmt_type = unseg | trackshots mode = BODY_25 | BODY_25_ALL | BODY_135 num_frames various version information and command line flag information ... /timeline - Contains shots if trackshots, otherwise if unseg contains poses directly. /timeline/shot0 - A single shot containing poses and with attributes start_frame and end_frame. This interval is closed at the beginning and open and the end, as with Python slices so that num_frames = end_frame - start_frame. /timeline/shot0/pose0 - A CSR sparse matrix[1] stored as a group. Has start_frame and end_frame. The shape of the matrix is (num_frames, limbs, 3). Each element of the matrix is a (x, y, c) tuple directly from OpenPose. CSR sparse matrix on Wikipedia","title":"Formats"},{"location":"identification/","text":"Person identification # There are two major approaches to identification: Direct comparison with one or more references. For each identified reference, multiple images may be supplied. First performing clustering and then either: Identifying clusters by comparison with one or more references. Identifying clusters manually by inspection of prototypical images. Construction a face reference library # A face reference library is simply a directory containing a number of directories, each containing photos containing the face of some individual of interest. Each directory should have a unique name which will be given used as a label for that person. All images should be JPEGs containing exactly one face. For example: myref \u251c\u2500\u2500 bill \u2502 \u2514\u2500\u2500 billphoto.jpg \u2514\u2500\u2500 ben \u2514\u2500\u2500 benphoto.jpeg By convention, we can use Wikidata identifiers to identify people: myref \u251c\u2500\u2500 Q230739 \u2502 \u2514\u2500\u2500 Katie%20Couric%20VF%202012%20Shankbone%202.JPG \u251c\u2500\u2500 Q6271597 \u2502 \u2514\u2500\u2500 Jon%20Snow2.jpg \u2514\u2500\u2500 Q929985 \u2514\u2500\u2500 Alex%20Trebek%20%28May%2021%2C%202012%29.jpg.jpg SkelShop can help quickly construct this specific type of reference. For example the above reference can be created by first creating a file people.txt containing: Q230739 Q6271597 Q929985 And then simply running: $ skelshop iden buildrefs entities people.txt myref wikidata Embedding your face library in advance # At your option, you can embed the reference library using skelshop iden embedrefs myref/ myref.h5 . All commands accepting a reference can then be given the output HDF5 file instead of the input directory. Direct comparison # Once we have obtained either a full or sparse set of face embeddings , we can identify people in each shot by direct comparison with our reference like so for a sparse embedding dump skelshop iden idsegssparse myref/ path/to/scenes.csv path/to/sparsefacedump.h5 outputids.csv . Or like so for a full embedding dump: skelshop iden idsegsfull myref/ path/to/scenes.csv path/to/sparsefacedump.h5 outputids.csv . Making a corpus description file # Up until now, we have been dealing with commands that process either only a single video at a time, or process information associated with a single video, such as skeleton dumps, at a time. However, when we perform face clustering we would like to deal data from multiple videos. To do this, we need to (usually automatically, e.g. in Snakemake create a CSV file for our corpus. Here is an example: video,group,group_typ,faces,segsout,untracked_skels,tracked_skels,bestcands /path/to/video.mp4,/path/to/scenes,ffprobe|psd,/path/to/facedump.h5,/path/to/clustering.output.csv,/path/to/untracked.skels.h5,/path/to/tracked.skels.h5,/path/to/bestcands.csv Paths in the corpus description file can be absolute or relative. If they are relative, you must pass in --corpus-base at the same time as passing in the CSV file. Clustering # The best family of clustering algorithm to use in situations like ours, where clusters of high density are separated by areas of areas of low density are density-based clustering algorithms. The classic density-based clustering algorithm is DBSCAN. However, the version of DBSCAN included in sklearn has issues with high memory consumption 1 . sklearn also contains the OPTICS clustering method, which is capable of producing clusterings identical to DBSCAN. Sklearn's OPTICS implementation doesn't have memory consumption issues. By default sklearn will use its own tree based spacial indexes to accelerate the distance queries underlying DBSCAN like algorithms, however, these indices are unable to deal well with hundreds of thousands of high dimensional vectors, like the face embeddings we are dealing with here. To mitigate this problem, it is possible to first use an library with an accelerated index designed for dealing with large numbers of high dimensional points to construct a (usually approximate) K-nearest-neighbours graph and then run clustering on this 2 . Running DBSCAN on a K-nearest-neighbours graph is possible, however DBSCAN is based on radius queries, we we must be careful to pick a high enough K that, most of the time most of the points within the query radius as in the Knn graph. As an added advantage, RNN-DBSCAN 3 only has a single parameter, K, which is the theshold of reverse nearest neighbours, but can be though of as the similar to DBSCAN's minPts - 1, making tuning easier. In our case if we take f faces per segment then we can choose this parameter as K = nf - 1 where n is the minimum number of appearances in segments someone should make to be assigned a cluster rather than treated as noise. 2 is a reasonable value for n, and so K = 5 is a good value for RNN-DBSCAN assuming you leave f at its default of 3. Currently, RNN-DBSCAN with pynndescent is the recommended approach. So the recommended command is: skelshop iden clus fixed --proto-out protos --model-out model.pkl --ann-lib pynndescent --algorithm rnn-dbscan --knn 5 path/to/corpus.description.csv . The --proto-out option gives a path to write prototypes/exemplars from each cluster, usable to align the clusters after-the-fact, while the --model-out option shows where to dump the model, which provides an alternative way of achieving the same thing, as explained in the next section. Labelling clusters # There are a three approaches to labelling clusters: Manual labelling based on saved prototypes Labelling by comparing a reference to saved prototypes Labelling by comparing a reference to a saved kNN+clustering model (only supported for pynndescent+RNN-DBSCAN) In the first case of manual labelling, we usually want to dump the images of the prototypes for each cluster like so: skelshop iden writeprotos protos path/to/corpus.description.csv path/to/proto/images . You can either install sxiv and use skelshop iden whoisthis path/to/proto/images cluster.identifications.csv to interactively labels the clusters, or manually then use an image viewer to view the cluster prototypes and then create a CSV file cluster.identifications.csv in the following format: label,clus Q42,c0 In the second case of prototype-based automatic labelling, we can use: skelshop iden idclus myref.h5 protos path/to/corpus.description.csv cluster.identifications.csv . Finally, to use a saved RNN-DBSCAN model skelshop iden idrnnclus myref.h5 model.pkl cluster.identifications.csv . After completing any of these three approaches, the labelled clusters can then be applied to the clustering outputs to produce a CSV with a mix of identities and clusters like so: skelshop iden applymap clustering.output.csv cluster.identifications.csv outputids.csv . This is discussed further in this StackOverflow discussion . \u21a9 The wrapper code enabling this, as well as the implementation of RNN-DBSCAN is found in sklearn-ann . \u21a9 A. Bryant and K. Cios, \"RNN-DBSCAN: A Density-Based Clustering Algorithm Using Reverse Nearest Neighbor Density Estimates,\" in IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 6, pp. 1109-1121, 1 June 2018, doi: 10.1109/TKDE.2017.2787640. \u21a9","title":"Person identification"},{"location":"identification/#person-identification","text":"There are two major approaches to identification: Direct comparison with one or more references. For each identified reference, multiple images may be supplied. First performing clustering and then either: Identifying clusters by comparison with one or more references. Identifying clusters manually by inspection of prototypical images.","title":"Person identification"},{"location":"identification/#construction-a-face-reference-library","text":"A face reference library is simply a directory containing a number of directories, each containing photos containing the face of some individual of interest. Each directory should have a unique name which will be given used as a label for that person. All images should be JPEGs containing exactly one face. For example: myref \u251c\u2500\u2500 bill \u2502 \u2514\u2500\u2500 billphoto.jpg \u2514\u2500\u2500 ben \u2514\u2500\u2500 benphoto.jpeg By convention, we can use Wikidata identifiers to identify people: myref \u251c\u2500\u2500 Q230739 \u2502 \u2514\u2500\u2500 Katie%20Couric%20VF%202012%20Shankbone%202.JPG \u251c\u2500\u2500 Q6271597 \u2502 \u2514\u2500\u2500 Jon%20Snow2.jpg \u2514\u2500\u2500 Q929985 \u2514\u2500\u2500 Alex%20Trebek%20%28May%2021%2C%202012%29.jpg.jpg SkelShop can help quickly construct this specific type of reference. For example the above reference can be created by first creating a file people.txt containing: Q230739 Q6271597 Q929985 And then simply running: $ skelshop iden buildrefs entities people.txt myref wikidata","title":"Construction a face reference library"},{"location":"identification/#embedding-your-face-library-in-advance","text":"At your option, you can embed the reference library using skelshop iden embedrefs myref/ myref.h5 . All commands accepting a reference can then be given the output HDF5 file instead of the input directory.","title":"Embedding your face library in advance"},{"location":"identification/#direct-comparison","text":"Once we have obtained either a full or sparse set of face embeddings , we can identify people in each shot by direct comparison with our reference like so for a sparse embedding dump skelshop iden idsegssparse myref/ path/to/scenes.csv path/to/sparsefacedump.h5 outputids.csv . Or like so for a full embedding dump: skelshop iden idsegsfull myref/ path/to/scenes.csv path/to/sparsefacedump.h5 outputids.csv .","title":"Direct comparison"},{"location":"identification/#making-a-corpus-description-file","text":"Up until now, we have been dealing with commands that process either only a single video at a time, or process information associated with a single video, such as skeleton dumps, at a time. However, when we perform face clustering we would like to deal data from multiple videos. To do this, we need to (usually automatically, e.g. in Snakemake create a CSV file for our corpus. Here is an example: video,group,group_typ,faces,segsout,untracked_skels,tracked_skels,bestcands /path/to/video.mp4,/path/to/scenes,ffprobe|psd,/path/to/facedump.h5,/path/to/clustering.output.csv,/path/to/untracked.skels.h5,/path/to/tracked.skels.h5,/path/to/bestcands.csv Paths in the corpus description file can be absolute or relative. If they are relative, you must pass in --corpus-base at the same time as passing in the CSV file.","title":"Making a corpus description file"},{"location":"identification/#clustering","text":"The best family of clustering algorithm to use in situations like ours, where clusters of high density are separated by areas of areas of low density are density-based clustering algorithms. The classic density-based clustering algorithm is DBSCAN. However, the version of DBSCAN included in sklearn has issues with high memory consumption 1 . sklearn also contains the OPTICS clustering method, which is capable of producing clusterings identical to DBSCAN. Sklearn's OPTICS implementation doesn't have memory consumption issues. By default sklearn will use its own tree based spacial indexes to accelerate the distance queries underlying DBSCAN like algorithms, however, these indices are unable to deal well with hundreds of thousands of high dimensional vectors, like the face embeddings we are dealing with here. To mitigate this problem, it is possible to first use an library with an accelerated index designed for dealing with large numbers of high dimensional points to construct a (usually approximate) K-nearest-neighbours graph and then run clustering on this 2 . Running DBSCAN on a K-nearest-neighbours graph is possible, however DBSCAN is based on radius queries, we we must be careful to pick a high enough K that, most of the time most of the points within the query radius as in the Knn graph. As an added advantage, RNN-DBSCAN 3 only has a single parameter, K, which is the theshold of reverse nearest neighbours, but can be though of as the similar to DBSCAN's minPts - 1, making tuning easier. In our case if we take f faces per segment then we can choose this parameter as K = nf - 1 where n is the minimum number of appearances in segments someone should make to be assigned a cluster rather than treated as noise. 2 is a reasonable value for n, and so K = 5 is a good value for RNN-DBSCAN assuming you leave f at its default of 3. Currently, RNN-DBSCAN with pynndescent is the recommended approach. So the recommended command is: skelshop iden clus fixed --proto-out protos --model-out model.pkl --ann-lib pynndescent --algorithm rnn-dbscan --knn 5 path/to/corpus.description.csv . The --proto-out option gives a path to write prototypes/exemplars from each cluster, usable to align the clusters after-the-fact, while the --model-out option shows where to dump the model, which provides an alternative way of achieving the same thing, as explained in the next section.","title":"Clustering"},{"location":"identification/#labelling-clusters","text":"There are a three approaches to labelling clusters: Manual labelling based on saved prototypes Labelling by comparing a reference to saved prototypes Labelling by comparing a reference to a saved kNN+clustering model (only supported for pynndescent+RNN-DBSCAN) In the first case of manual labelling, we usually want to dump the images of the prototypes for each cluster like so: skelshop iden writeprotos protos path/to/corpus.description.csv path/to/proto/images . You can either install sxiv and use skelshop iden whoisthis path/to/proto/images cluster.identifications.csv to interactively labels the clusters, or manually then use an image viewer to view the cluster prototypes and then create a CSV file cluster.identifications.csv in the following format: label,clus Q42,c0 In the second case of prototype-based automatic labelling, we can use: skelshop iden idclus myref.h5 protos path/to/corpus.description.csv cluster.identifications.csv . Finally, to use a saved RNN-DBSCAN model skelshop iden idrnnclus myref.h5 model.pkl cluster.identifications.csv . After completing any of these three approaches, the labelled clusters can then be applied to the clustering outputs to produce a CSV with a mix of identities and clusters like so: skelshop iden applymap clustering.output.csv cluster.identifications.csv outputids.csv . This is discussed further in this StackOverflow discussion . \u21a9 The wrapper code enabling this, as well as the implementation of RNN-DBSCAN is found in sklearn-ann . \u21a9 A. Bryant and K. Cios, \"RNN-DBSCAN: A Density-Based Clustering Algorithm Using Reverse Nearest Neighbor Density Estimates,\" in IEEE Transactions on Knowledge and Data Engineering, vol. 30, no. 6, pp. 1109-1121, 1 June 2018, doi: 10.1109/TKDE.2017.2787640. \u21a9","title":"Labelling clusters"},{"location":"io/","text":"Reading and writing the formats from your own code # There are tools to read skeletons and face embeddings from your own code. Here is an example of reading from a tracked skeleton file: import h5py from skelshop.io import ShotSegmentedReader with h5py.File(\"/path/to/my/skels.h5\", \"r\") as skels_f: for shot in ShotSegmentedReader(skels_f): for skel_id, skel in shot: print(skel_id, skel) Reference # skelshop.io.UnsegmentedWriter # Write a skeleton dump without any shot segmentation. This typically implies that poses are not tracked. __init__ ( self , h5f , num_kps = None , ** create_kwargs ) special # Constructs an unsegmented pose writer Source code in skelshop/io.py def __init__ ( self , h5f : h5py . File , num_kps = None , ** create_kwargs ): \"\"\" Constructs an unsegmented pose writer \"\"\" self . h5f = h5f self . num_kps = num_kps self . timeline_grp = self . h5f . create_group ( \"/timeline\" , track_order = True ) self . pose_grps : Dict [ int , List [ Any ]] = {} self . start_frame = 0 self . create_kwargs = create_kwargs add_pose ( self , frame_num , pose_id , pose ) # Add a pose Source code in skelshop/io.py def add_pose ( self , frame_num : int , pose_id : int , pose : ndarray ): \"\"\" Add a pose \"\"\" pose_grp , data , indices , indptr , last_frame_num = self . _pose_grp ( pose_id , frame_num ) new_rows = frame_num - last_frame_num add_empty_rows_grp ( indptr , data , new_rows ) new_data = [] new_indices = [] for limb_idx , limb in get_pose_nz ( pose ): new_data . append ( limb ) new_indices . append ( limb_idx ) if new_data : grow_ds ( data , len ( new_data )) grow_ds ( indices , len ( new_indices )) data [ - len ( new_data ) :] = new_data indices [ - len ( new_indices ) :] = new_indices self . pose_grps [ pose_id ][ - 1 ] = frame_num end_shot ( self ) # Ends a shot. This should only be called once at the end of writing. Source code in skelshop/io.py def end_shot ( self ): \"\"\" Ends a shot. This should only be called once at the end of writing. \"\"\" self . timeline_grp . attrs [ \"start_frame\" ] = self . start_frame timeline_last_frame_num = 0 for pose_grp , data , indices , indptr , last_frame_num in self . pose_grps . values (): add_empty_rows_grp ( indptr , data , 1 ) pose_grp . attrs [ \"end_frame\" ] = last_frame_num + 1 timeline_last_frame_num = max ( timeline_last_frame_num , last_frame_num ) self . timeline_grp . attrs [ \"end_frame\" ] = timeline_last_frame_num + 1 start_shot ( self , start_frame = 0 ) # Start a shot. This should only be called once at the beginning of writing. Source code in skelshop/io.py def start_shot ( self , start_frame : int = 0 ): \"\"\" Start a shot. This should only be called once at the beginning of writing. \"\"\" self . start_frame = start_frame skelshop.io.ShotSegmentedWriter # Write a skeleton dump with any shot segmentation. This typically implies that poses are are tracked. __init__ ( self , h5f , num_kps = None , ** create_kwargs ) special # Constructs a shot segmented writer Source code in skelshop/io.py def __init__ ( self , h5f : h5py . File , num_kps = None , ** create_kwargs ): \"\"\" Constructs a shot segmented writer \"\"\" self . h5f = h5f self . num_kps = num_kps self . h5f . create_group ( \"/timeline\" , track_order = True ) self . pose_data : Dict [ int , Dict [ int , ndarray ]] = {} self . pose_starts : Dict [ int , int ] = {} self . pose_ends : Dict [ int , int ] = {} self . shot_idx = 0 self . shot_start = 0 self . last_frame = 0 self . create_kwargs = create_kwargs add_pose ( self , frame_num , pose_id , pose ) # Add a pose Source code in skelshop/io.py def add_pose ( self , frame_num : int , pose_id : int , pose : ndarray ): \"\"\" Add a pose \"\"\" if pose_id not in self . pose_data : self . pose_starts [ pose_id ] = frame_num self . pose_data [ pose_id ] = {} self . pose_data [ pose_id ][ frame_num ] = pose self . pose_ends [ pose_id ] = frame_num self . last_frame = frame_num end_shot ( self ) # End the current shot Source code in skelshop/io.py def end_shot ( self ): \"\"\" End the current shot \"\"\" shot_grp = self . h5f . create_group ( f \"/timeline/shot { self . shot_idx } \" , track_order = True ) shot_grp . attrs [ \"start_frame\" ] = self . shot_start shot_grp . attrs [ \"end_frame\" ] = self . last_frame + 1 for pose_id , poses in self . pose_data . items (): data : List [ ndarray ] = [] indices : List [ int ] = [] indptr : List [ int ] = [] pose_first_frame = self . pose_starts [ pose_id ] pose_last_frame = self . pose_ends [ pose_id ] + 1 last_frame_num = pose_first_frame - 1 def add_empty_rows ( num_rows ): for _ in range ( num_rows ): indptr . append ( len ( data )) for frame_num , pose in poses . items (): add_empty_rows ( frame_num - last_frame_num ) for limb_idx , limb in get_pose_nz ( pose ): data . append ( limb ) indices . append ( limb_idx ) last_frame_num = frame_num # Extra empty row to insert final nnz entry add_empty_rows ( 1 ) pose_group = create_csr ( self . h5f , f \"/timeline/shot { self . shot_idx } /pose { pose_id } \" , self . num_kps , data = data , indices = indices , indptr = indptr , ** self . create_kwargs , ) pose_group . attrs [ \"start_frame\" ] = pose_first_frame pose_group . attrs [ \"end_frame\" ] = pose_last_frame self . pose_data = {} self . shot_idx += 1 self . shot_start = self . last_frame + 1 register_frame ( self , frame_num ) # Register frame_num as existing within the current shot Source code in skelshop/io.py def register_frame ( self , frame_num : int ): \"\"\" Register frame_num as existing within the current shot \"\"\" self . last_frame = frame_num start_shot ( self , start_frame = None ) # Start a new shot Source code in skelshop/io.py def start_shot ( self , start_frame = None ): \"\"\" Start a new shot \"\"\" if start_frame is not None : self . shot_start = start_frame skelshop.io.ShotSegmentedReader # Reads a shot segmented skeleton dump. __init__ ( self , h5f , bundle_cls =< class ' skelshop . pose . DumpReaderPoseBundle '>, infinite=False) special # Constructs the reader from a HDF5 file and. If infinite is True, all shot iterators will terminate with a final empty shot which infintely yields empty pose bundles. Source code in skelshop/io.py def __init__ ( self , h5f : h5py . File , bundle_cls = DumpReaderPoseBundle , infinite = False ): \"\"\" Constructs the reader from a HDF5 file and. If `infinite` is True, all shot iterators will terminate with a final empty shot which infintely yields empty pose bundles. \"\"\" self . h5f = h5f self . limbs = self . h5f . attrs [ \"limbs\" ] assert self . h5f . attrs [ \"fmt_type\" ] == \"trackshots\" self . mk_bundle = partial ( bundle_cls , cls = POSE_CLASSES [ self . h5f . attrs [ \"mode\" ]]) self . empty_bundle = self . mk_bundle ({}) self . infinite = infinite __iter__ ( self ) special # Returns a shot iterator of all shots Source code in skelshop/io.py def __iter__ ( self ): \"\"\" Returns a shot iterator of all shots \"\"\" for shot_idx , shot_name , start_frame , end_frame , mk_shot in self . _iter (): yield mk_shot () iter_from_frame ( self , start_frame ) # Returns a shot iterator starting from frame 0-index start_frame . Source code in skelshop/io.py def iter_from_frame ( self , start_frame : int ): \"\"\" Returns a shot iterator starting from frame 0-index `start_frame`. \"\"\" started = False for ( shot_idx , shot_name , shot_start_frame , shot_end_frame , mk_shot , ) in self . _iter ( start_frame ): if started : yield mk_shot () elif shot_start_frame <= start_frame and ( shot_end_frame is None or start_frame < shot_end_frame ): yield mk_shot () started = True iter_from_shot ( self , start_shot ) # Returns a shot iterator starting from shot 0-index start_shot . Source code in skelshop/io.py def iter_from_shot ( self , start_shot : int ): \"\"\" Returns a shot iterator starting from shot 0-index `start_shot`. \"\"\" for shot_idx , shot_name , start_frame , end_frame , mk_shot in self . _iter (): if shot_idx >= start_shot : yield mk_shot () skelshop.io.UnsegmentedReader # Reads a non-shot segmented skeleton dump. __iter__ ( self ) special # Returns a pose bundle iterator of all frames. Source code in skelshop/io.py def __iter__ ( self ): \"\"\" Returns a pose bundle iterator of all frames. \"\"\" return iter ( self . shot_reader ) iter_from ( self , start_frame ) # Returns a pose bundle iterator starting at frame start_frame . Source code in skelshop/io.py def iter_from ( self , start_frame : int ): \"\"\" Returns a pose bundle iterator starting at frame `start_frame`. \"\"\" return self . shot_reader . iter_from ( start_frame ) skelshop.io.ShotReader # A reader for a single shot. Typically this is returned from ShotSegmentedReader. __iter__ ( self ) special # Returns a pose bundle iterator of all frames. Source code in skelshop/io.py def __iter__ ( self ): \"\"\" Returns a pose bundle iterator of all frames. \"\"\" return self . iter_from ( self . start_frame ) iter_from ( self , start_frame ) # Returns a pose bundle iterator starting at frame start_frame . Source code in skelshop/io.py def iter_from ( self , start_frame ): \"\"\" Returns a pose bundle iterator starting at frame `start_frame`. \"\"\" for frame in range ( start_frame , self . end_frame ): bundle = {} for pose_num , start_frame , end_frame , sparse_pose in self . poses : if start_frame <= frame < end_frame : row_num = frame - start_frame bundle [ pose_num ] = sparse_pose . get_row ( row_num ) yield self . mk_bundle ( bundle ) skelshop.io.AsIfTracked # Adapter wrapper for readers producing UntrackedDumpReaderPoseBundle such as UnsegmentedReader by default that makes them appear to produce bundles like TrackedDumpReaderPoseBundle. skelshop.io.AsIfSingleShot # Adapter wrapper for ShotSegmentedReader to make it act more like an UnsegmentedReader.","title":"Reading and writing the formats from your own code"},{"location":"io/#reading-and-writing-the-formats-from-your-own-code","text":"There are tools to read skeletons and face embeddings from your own code. Here is an example of reading from a tracked skeleton file: import h5py from skelshop.io import ShotSegmentedReader with h5py.File(\"/path/to/my/skels.h5\", \"r\") as skels_f: for shot in ShotSegmentedReader(skels_f): for skel_id, skel in shot: print(skel_id, skel)","title":"Reading and writing the formats from your own code"},{"location":"io/#reference","text":"","title":"Reference"},{"location":"io/#skelshop.io.UnsegmentedWriter","text":"Write a skeleton dump without any shot segmentation. This typically implies that poses are not tracked.","title":"UnsegmentedWriter"},{"location":"io/#skelshop.io.UnsegmentedWriter.__init__","text":"Constructs an unsegmented pose writer Source code in skelshop/io.py def __init__ ( self , h5f : h5py . File , num_kps = None , ** create_kwargs ): \"\"\" Constructs an unsegmented pose writer \"\"\" self . h5f = h5f self . num_kps = num_kps self . timeline_grp = self . h5f . create_group ( \"/timeline\" , track_order = True ) self . pose_grps : Dict [ int , List [ Any ]] = {} self . start_frame = 0 self . create_kwargs = create_kwargs","title":"__init__()"},{"location":"io/#skelshop.io.UnsegmentedWriter.add_pose","text":"Add a pose Source code in skelshop/io.py def add_pose ( self , frame_num : int , pose_id : int , pose : ndarray ): \"\"\" Add a pose \"\"\" pose_grp , data , indices , indptr , last_frame_num = self . _pose_grp ( pose_id , frame_num ) new_rows = frame_num - last_frame_num add_empty_rows_grp ( indptr , data , new_rows ) new_data = [] new_indices = [] for limb_idx , limb in get_pose_nz ( pose ): new_data . append ( limb ) new_indices . append ( limb_idx ) if new_data : grow_ds ( data , len ( new_data )) grow_ds ( indices , len ( new_indices )) data [ - len ( new_data ) :] = new_data indices [ - len ( new_indices ) :] = new_indices self . pose_grps [ pose_id ][ - 1 ] = frame_num","title":"add_pose()"},{"location":"io/#skelshop.io.UnsegmentedWriter.end_shot","text":"Ends a shot. This should only be called once at the end of writing. Source code in skelshop/io.py def end_shot ( self ): \"\"\" Ends a shot. This should only be called once at the end of writing. \"\"\" self . timeline_grp . attrs [ \"start_frame\" ] = self . start_frame timeline_last_frame_num = 0 for pose_grp , data , indices , indptr , last_frame_num in self . pose_grps . values (): add_empty_rows_grp ( indptr , data , 1 ) pose_grp . attrs [ \"end_frame\" ] = last_frame_num + 1 timeline_last_frame_num = max ( timeline_last_frame_num , last_frame_num ) self . timeline_grp . attrs [ \"end_frame\" ] = timeline_last_frame_num + 1","title":"end_shot()"},{"location":"io/#skelshop.io.UnsegmentedWriter.start_shot","text":"Start a shot. This should only be called once at the beginning of writing. Source code in skelshop/io.py def start_shot ( self , start_frame : int = 0 ): \"\"\" Start a shot. This should only be called once at the beginning of writing. \"\"\" self . start_frame = start_frame","title":"start_shot()"},{"location":"io/#skelshop.io.ShotSegmentedWriter","text":"Write a skeleton dump with any shot segmentation. This typically implies that poses are are tracked.","title":"ShotSegmentedWriter"},{"location":"io/#skelshop.io.ShotSegmentedWriter.__init__","text":"Constructs a shot segmented writer Source code in skelshop/io.py def __init__ ( self , h5f : h5py . File , num_kps = None , ** create_kwargs ): \"\"\" Constructs a shot segmented writer \"\"\" self . h5f = h5f self . num_kps = num_kps self . h5f . create_group ( \"/timeline\" , track_order = True ) self . pose_data : Dict [ int , Dict [ int , ndarray ]] = {} self . pose_starts : Dict [ int , int ] = {} self . pose_ends : Dict [ int , int ] = {} self . shot_idx = 0 self . shot_start = 0 self . last_frame = 0 self . create_kwargs = create_kwargs","title":"__init__()"},{"location":"io/#skelshop.io.ShotSegmentedWriter.add_pose","text":"Add a pose Source code in skelshop/io.py def add_pose ( self , frame_num : int , pose_id : int , pose : ndarray ): \"\"\" Add a pose \"\"\" if pose_id not in self . pose_data : self . pose_starts [ pose_id ] = frame_num self . pose_data [ pose_id ] = {} self . pose_data [ pose_id ][ frame_num ] = pose self . pose_ends [ pose_id ] = frame_num self . last_frame = frame_num","title":"add_pose()"},{"location":"io/#skelshop.io.ShotSegmentedWriter.end_shot","text":"End the current shot Source code in skelshop/io.py def end_shot ( self ): \"\"\" End the current shot \"\"\" shot_grp = self . h5f . create_group ( f \"/timeline/shot { self . shot_idx } \" , track_order = True ) shot_grp . attrs [ \"start_frame\" ] = self . shot_start shot_grp . attrs [ \"end_frame\" ] = self . last_frame + 1 for pose_id , poses in self . pose_data . items (): data : List [ ndarray ] = [] indices : List [ int ] = [] indptr : List [ int ] = [] pose_first_frame = self . pose_starts [ pose_id ] pose_last_frame = self . pose_ends [ pose_id ] + 1 last_frame_num = pose_first_frame - 1 def add_empty_rows ( num_rows ): for _ in range ( num_rows ): indptr . append ( len ( data )) for frame_num , pose in poses . items (): add_empty_rows ( frame_num - last_frame_num ) for limb_idx , limb in get_pose_nz ( pose ): data . append ( limb ) indices . append ( limb_idx ) last_frame_num = frame_num # Extra empty row to insert final nnz entry add_empty_rows ( 1 ) pose_group = create_csr ( self . h5f , f \"/timeline/shot { self . shot_idx } /pose { pose_id } \" , self . num_kps , data = data , indices = indices , indptr = indptr , ** self . create_kwargs , ) pose_group . attrs [ \"start_frame\" ] = pose_first_frame pose_group . attrs [ \"end_frame\" ] = pose_last_frame self . pose_data = {} self . shot_idx += 1 self . shot_start = self . last_frame + 1","title":"end_shot()"},{"location":"io/#skelshop.io.ShotSegmentedWriter.register_frame","text":"Register frame_num as existing within the current shot Source code in skelshop/io.py def register_frame ( self , frame_num : int ): \"\"\" Register frame_num as existing within the current shot \"\"\" self . last_frame = frame_num","title":"register_frame()"},{"location":"io/#skelshop.io.ShotSegmentedWriter.start_shot","text":"Start a new shot Source code in skelshop/io.py def start_shot ( self , start_frame = None ): \"\"\" Start a new shot \"\"\" if start_frame is not None : self . shot_start = start_frame","title":"start_shot()"},{"location":"io/#skelshop.io.ShotSegmentedReader","text":"Reads a shot segmented skeleton dump.","title":"ShotSegmentedReader"},{"location":"io/#skelshop.io.ShotSegmentedReader.__init__","text":"Constructs the reader from a HDF5 file and. If infinite is True, all shot iterators will terminate with a final empty shot which infintely yields empty pose bundles. Source code in skelshop/io.py def __init__ ( self , h5f : h5py . File , bundle_cls = DumpReaderPoseBundle , infinite = False ): \"\"\" Constructs the reader from a HDF5 file and. If `infinite` is True, all shot iterators will terminate with a final empty shot which infintely yields empty pose bundles. \"\"\" self . h5f = h5f self . limbs = self . h5f . attrs [ \"limbs\" ] assert self . h5f . attrs [ \"fmt_type\" ] == \"trackshots\" self . mk_bundle = partial ( bundle_cls , cls = POSE_CLASSES [ self . h5f . attrs [ \"mode\" ]]) self . empty_bundle = self . mk_bundle ({}) self . infinite = infinite","title":"__init__()"},{"location":"io/#skelshop.io.ShotSegmentedReader.__iter__","text":"Returns a shot iterator of all shots Source code in skelshop/io.py def __iter__ ( self ): \"\"\" Returns a shot iterator of all shots \"\"\" for shot_idx , shot_name , start_frame , end_frame , mk_shot in self . _iter (): yield mk_shot ()","title":"__iter__()"},{"location":"io/#skelshop.io.ShotSegmentedReader.iter_from_frame","text":"Returns a shot iterator starting from frame 0-index start_frame . Source code in skelshop/io.py def iter_from_frame ( self , start_frame : int ): \"\"\" Returns a shot iterator starting from frame 0-index `start_frame`. \"\"\" started = False for ( shot_idx , shot_name , shot_start_frame , shot_end_frame , mk_shot , ) in self . _iter ( start_frame ): if started : yield mk_shot () elif shot_start_frame <= start_frame and ( shot_end_frame is None or start_frame < shot_end_frame ): yield mk_shot () started = True","title":"iter_from_frame()"},{"location":"io/#skelshop.io.ShotSegmentedReader.iter_from_shot","text":"Returns a shot iterator starting from shot 0-index start_shot . Source code in skelshop/io.py def iter_from_shot ( self , start_shot : int ): \"\"\" Returns a shot iterator starting from shot 0-index `start_shot`. \"\"\" for shot_idx , shot_name , start_frame , end_frame , mk_shot in self . _iter (): if shot_idx >= start_shot : yield mk_shot ()","title":"iter_from_shot()"},{"location":"io/#skelshop.io.UnsegmentedReader","text":"Reads a non-shot segmented skeleton dump.","title":"UnsegmentedReader"},{"location":"io/#skelshop.io.UnsegmentedReader.__iter__","text":"Returns a pose bundle iterator of all frames. Source code in skelshop/io.py def __iter__ ( self ): \"\"\" Returns a pose bundle iterator of all frames. \"\"\" return iter ( self . shot_reader )","title":"__iter__()"},{"location":"io/#skelshop.io.UnsegmentedReader.iter_from","text":"Returns a pose bundle iterator starting at frame start_frame . Source code in skelshop/io.py def iter_from ( self , start_frame : int ): \"\"\" Returns a pose bundle iterator starting at frame `start_frame`. \"\"\" return self . shot_reader . iter_from ( start_frame )","title":"iter_from()"},{"location":"io/#skelshop.io.ShotReader","text":"A reader for a single shot. Typically this is returned from ShotSegmentedReader.","title":"ShotReader"},{"location":"io/#skelshop.io.ShotReader.__iter__","text":"Returns a pose bundle iterator of all frames. Source code in skelshop/io.py def __iter__ ( self ): \"\"\" Returns a pose bundle iterator of all frames. \"\"\" return self . iter_from ( self . start_frame )","title":"__iter__()"},{"location":"io/#skelshop.io.ShotReader.iter_from","text":"Returns a pose bundle iterator starting at frame start_frame . Source code in skelshop/io.py def iter_from ( self , start_frame ): \"\"\" Returns a pose bundle iterator starting at frame `start_frame`. \"\"\" for frame in range ( start_frame , self . end_frame ): bundle = {} for pose_num , start_frame , end_frame , sparse_pose in self . poses : if start_frame <= frame < end_frame : row_num = frame - start_frame bundle [ pose_num ] = sparse_pose . get_row ( row_num ) yield self . mk_bundle ( bundle )","title":"iter_from()"},{"location":"io/#skelshop.io.AsIfTracked","text":"Adapter wrapper for readers producing UntrackedDumpReaderPoseBundle such as UnsegmentedReader by default that makes them appear to produce bundles like TrackedDumpReaderPoseBundle.","title":"AsIfTracked"},{"location":"io/#skelshop.io.AsIfSingleShot","text":"Adapter wrapper for ShotSegmentedReader to make it act more like an UnsegmentedReader.","title":"AsIfSingleShot"},{"location":"pipeline-internals/","text":"The stage interface is quite simple. Each stage acts as an iterator, typically yielding some kind of pose bundle. A pose bundle is an iterator of skeletons, either with ids or not depending on whether it has been tracked. Each stage inherits from the PipelineStageBase abstract base class which includes also send_back to send back events to earlier stages in the pipeline. skelshop.pipebase.PipelineStageBase # The abstract base class for a pipeline stage. __next__ ( self ) special # Get the payload for the next frame Source code in skelshop/pipebase.py @abstractmethod def __next__ ( self ): \"\"\" Get the payload for the next frame \"\"\" ... send_back ( self , name , * args , ** kwargs ) # Send a message back down the pipeline by calling a method with name , *args , and `*kwargs Source code in skelshop/pipebase.py def send_back ( self , name : str , * args , ** kwargs ): \"\"\" Send a message back down the pipeline by calling a method with `name`, `*args`, and `*kwargs \"\"\" meth = getattr ( self , name , None ) if meth is not None : meth ( * args , ** kwargs ) return if self . prev is not None : self . prev . send_back ( name , * args , ** kwargs ) Events types in use through send_back # Currently cut event is sent back by any shot segmentation stage to the tracking stage, so that tracking can be reset. Each stage is free to deal with events as it wishes, e.g. a tracking stage attempting to track across shots could react differently to this event. A rewind event can be sent back so that a RewindStage will reverse a given number of frames in its buffer. Note that you must arrange for a RewindStage to be placed into the pipeline.","title":"Pipeline internals"},{"location":"pipeline-internals/#skelshop.pipebase.PipelineStageBase","text":"The abstract base class for a pipeline stage.","title":"PipelineStageBase"},{"location":"pipeline-internals/#skelshop.pipebase.PipelineStageBase.__next__","text":"Get the payload for the next frame Source code in skelshop/pipebase.py @abstractmethod def __next__ ( self ): \"\"\" Get the payload for the next frame \"\"\" ...","title":"__next__()"},{"location":"pipeline-internals/#skelshop.pipebase.PipelineStageBase.send_back","text":"Send a message back down the pipeline by calling a method with name , *args , and `*kwargs Source code in skelshop/pipebase.py def send_back ( self , name : str , * args , ** kwargs ): \"\"\" Send a message back down the pipeline by calling a method with `name`, `*args`, and `*kwargs \"\"\" meth = getattr ( self , name , None ) if meth is not None : meth ( * args , ** kwargs ) return if self . prev is not None : self . prev . send_back ( name , * args , ** kwargs )","title":"send_back()"},{"location":"pipeline-internals/#events-types-in-use-through-send_back","text":"Currently cut event is sent back by any shot segmentation stage to the tracking stage, so that tracking can be reset. Each stage is free to deal with events as it wishes, e.g. a tracking stage attempting to track across shots could react differently to this event. A rewind event can be sent back so that a RewindStage will reverse a given number of frames in its buffer. Note that you must arrange for a RewindStage to be placed into the pipeline.","title":"Events types in use through send_back"},{"location":"pipelines-overview/","text":"Skeleton pipeline # The overall skeleton pipeline goes like so But note these steps can be left out so we can dump first and do tracking later if we like e.g. first run And then later run Pipelines starting with the pose estimator are run using the dump command, while pipelines starting from existing pose dumps using the filter command. Which method is used for different . See next CLI examples and CLI reference . Face pipeline # The face pipeline can run in two modes. In the first mode, which is not recommended for most usages, dlib's face detection and face keypoint detection pipeline is used. There is some information about the dlib keypoint detection in this blog post . In the second mode, an existing pose dump including these 68-keypoints as estimated by OpenPose is used. The second is preferred in most situations. Dlib only pipeline: Skeleton-based pipeline:","title":"Pipelines overview"},{"location":"pipelines-overview/#skeleton-pipeline","text":"The overall skeleton pipeline goes like so But note these steps can be left out so we can dump first and do tracking later if we like e.g. first run And then later run Pipelines starting with the pose estimator are run using the dump command, while pipelines starting from existing pose dumps using the filter command. Which method is used for different . See next CLI examples and CLI reference .","title":"Skeleton pipeline"},{"location":"pipelines-overview/#face-pipeline","text":"The face pipeline can run in two modes. In the first mode, which is not recommended for most usages, dlib's face detection and face keypoint detection pipeline is used. There is some information about the dlib keypoint detection in this blog post . In the second mode, an existing pose dump including these 68-keypoints as estimated by OpenPose is used. The second is preferred in most situations. Dlib only pipeline: Skeleton-based pipeline:","title":"Face pipeline"},{"location":"pose-estimators/","text":"Pose estimators # Currently pose estimation is based on OpenPose. OpenPose # OpenPose can produce untracked poses. It can produce a variety of different keypoint models. Modes # Mode name Keypoints Body Hands Face BODY_25 25 Yes No No BODY_25_ALL 135 Yes Yes Yes BODY_25_HANDS 65 Yes Yes No FACE 70 No No Yes BODY_25_FACE 95 Yes No Yes","title":"Pose estimators"},{"location":"pose-estimators/#pose-estimators","text":"Currently pose estimation is based on OpenPose.","title":"Pose estimators"},{"location":"pose-estimators/#openpose","text":"OpenPose can produce untracked poses. It can produce a variety of different keypoint models.","title":"OpenPose"},{"location":"pose-estimators/#modes","text":"Mode name Keypoints Body Hands Face BODY_25 25 Yes No No BODY_25_ALL 135 Yes Yes Yes BODY_25_HANDS 65 Yes Yes No FACE 70 No No Yes BODY_25_FACE 95 Yes No Yes","title":"Modes"},{"location":"pose-trackers/","text":"For now please see the tracking documentation for developers .","title":"Within-shot pose trackers"},{"location":"shot-segmentors/","text":"Currently there are two shot detectors supported: PySceneDetect and ffprobe. They are run manually or through Snakemake . Samuel Albanie put together a (slightly old -- is it still valid?) comparison of shot segmentors including these two. The results of the shot segmentor must be given whenever within-shot pose tracking is performed. PySceneDetect # You can run this at the command line like so: $ poetry run scenedetect \\ --input /path/to/my/video.mp4 \\ --output /path/to/my/output/dir \\ detect-content \\ --min-scene-len 2s\\ list-scenes If you want to run this from your own Snakefile, you can use the wrapper script. Please refer to workflow/rules/skels.smk . ffprobe # It is recommended to use ffprobe via Snakemake. e.g. if you have a video /path/to/videos/myvideo.mp4 $ poetry run snakemake \\ /path/to/dumps/myvideo.ffprobe.scene.txt \\ --config \\ VIDEO_BASE=/path/to/videos \\ DUMP_BASE=/path/to/dumps If you want to run this from your own Snakefile, you can use the wrapper script. Please refer to workflow/rules/skels.smk . The black box segmentor # The black box segmentor runs after tracking, and does not use the video at all. It works by cutting a scene whenever the set of people in the frame change, as according to the output of the tracker. For this reason, in most cases, it will perform significantly worse than the other trackers. Its use is not recommended unless you don't have access to the original video anymore.","title":"Shot segmentors"},{"location":"shot-segmentors/#pyscenedetect","text":"You can run this at the command line like so: $ poetry run scenedetect \\ --input /path/to/my/video.mp4 \\ --output /path/to/my/output/dir \\ detect-content \\ --min-scene-len 2s\\ list-scenes If you want to run this from your own Snakefile, you can use the wrapper script. Please refer to workflow/rules/skels.smk .","title":"PySceneDetect"},{"location":"shot-segmentors/#ffprobe","text":"It is recommended to use ffprobe via Snakemake. e.g. if you have a video /path/to/videos/myvideo.mp4 $ poetry run snakemake \\ /path/to/dumps/myvideo.ffprobe.scene.txt \\ --config \\ VIDEO_BASE=/path/to/videos \\ DUMP_BASE=/path/to/dumps If you want to run this from your own Snakefile, you can use the wrapper script. Please refer to workflow/rules/skels.smk .","title":"ffprobe"},{"location":"shot-segmentors/#the-black-box-segmentor","text":"The black box segmentor runs after tracking, and does not use the video at all. It works by cutting a scene whenever the set of people in the frame change, as according to the output of the tracker. For this reason, in most cases, it will perform significantly worse than the other trackers. Its use is not recommended unless you don't have access to the original video anymore.","title":"The black box segmentor"},{"location":"snakemake/","text":"Workflow automation and orchestration, on your workstation and in HPC environments using Snakemake # SkelShop include tools for running extraction pipelines orchestrated using Snakemake. These workflows can be run on a single node, however more typically these would be run in a HPC environment, with a heterogeneous mix of GPU and CPU nodes being orchestrated by SLURM so as to enable skeleton/face extraction from a large video corpus in a reasonable amount of time. The intended environment for SkelShop to be run is in a SLURM-based HPC environment in a Singularity container. You can run Snakemake on one node (typically a login node, since no heavy computation is performed by this node) and the actual steps will run on different nodes chosen according to a JSON configuration file, all from a single Singularity container. This workflow is enabled by singslurm2 project, which is based on the Snakemake SLURM profile . Running Snakemake on a single node # Snakemake can be run on a single node, which might be appropriate if you have a very small video corpus or a lot of time(!) For example assuming you have followed the manual installation instruction and that you want to use 8 cores: $ cd /path/to/skelshop $ poetry run snakemake tracked_all \\ --cores 8 \\ --config \\ VIDEO_BASE=/path/to/my/video/corpus/ \\ DUMP_BASE=/path/to/my/dump/directory Running Snakemake on a SLURM cluster # First set up singslurm2: $ cd ~ $ git clone --recursive https://github.com/frankier/singslurm2.git Now you can download the Docker image with Singularity: $ singularity pull skelshop.sif docker://frankierr/skelshop:focal_nvcaffe Next, you need to create a JSON file specifying which type of nodes you would like to assign to different rules (steps in the workflow). There is an example for Case Western Reserve University SLURM cluster . See also the SLURM documentation and the SLURM Snakemake profile documentation for information on how to write this file. You can see the names of the steps in the workflow at any time by running: $ poetry run snakemake --list So for example you might: Download the example cluster configuration. $ wget https://github.com/frankier/skelshop/blob/master/contrib/slurm/skels.tracked.clusc.json Edit it if need be. Then run the following command after editing the placeholders (at least NUM_JOBS , SING_EXTRA_ARGS , VIDEO_BASE and DUMP_BASE : $ SIF_PATH=$(pwd)/skelshop.sif \\ SNAKEFILE=/opt/skelshop/workflow/Snakefile \\ CLUSC_CONF=$(pwd)/skels.tracked.clusc.json \\ NUM_JOBS=42 \\ SING_EXTRA_ARGS=\"--bind /path/to/my/extra/bind\" \\ ~/singslurm2/run.sh \\ tracked_all \\ --config \\ VIDEO_BASE=/path/to/my/video/corpus/ \\ DUMP_BASE=/path/to/my/dump/directory Please see the singslurm repository for more information about the environment variables passed to singslurm2/run.sh . Integrating SkelShop into your own pipelines # In case you are using SkelShop as part of a larger pipeline or want to further customise your workflow, you should write your own Snakefile. See the Snakemake documentation . You may like to use the rules and scripts from SkelShop. In this case the current best approach is to copy or symlink everything under workflow/rules and workflow/scripts into your own workflow directory. Other HPC utilities # There are some examples of how to run which are specific to the Case Western Reserve University SLURM cluster in the contrib/slurm directory .","title":"Automation with snakemake"},{"location":"snakemake/#workflow-automation-and-orchestration-on-your-workstation-and-in-hpc-environments-using-snakemake","text":"SkelShop include tools for running extraction pipelines orchestrated using Snakemake. These workflows can be run on a single node, however more typically these would be run in a HPC environment, with a heterogeneous mix of GPU and CPU nodes being orchestrated by SLURM so as to enable skeleton/face extraction from a large video corpus in a reasonable amount of time. The intended environment for SkelShop to be run is in a SLURM-based HPC environment in a Singularity container. You can run Snakemake on one node (typically a login node, since no heavy computation is performed by this node) and the actual steps will run on different nodes chosen according to a JSON configuration file, all from a single Singularity container. This workflow is enabled by singslurm2 project, which is based on the Snakemake SLURM profile .","title":"Workflow automation and orchestration, on your workstation and in HPC environments using Snakemake"},{"location":"snakemake/#running-snakemake-on-a-single-node","text":"Snakemake can be run on a single node, which might be appropriate if you have a very small video corpus or a lot of time(!) For example assuming you have followed the manual installation instruction and that you want to use 8 cores: $ cd /path/to/skelshop $ poetry run snakemake tracked_all \\ --cores 8 \\ --config \\ VIDEO_BASE=/path/to/my/video/corpus/ \\ DUMP_BASE=/path/to/my/dump/directory","title":"Running Snakemake on a single node"},{"location":"snakemake/#running-snakemake-on-a-slurm-cluster","text":"First set up singslurm2: $ cd ~ $ git clone --recursive https://github.com/frankier/singslurm2.git Now you can download the Docker image with Singularity: $ singularity pull skelshop.sif docker://frankierr/skelshop:focal_nvcaffe Next, you need to create a JSON file specifying which type of nodes you would like to assign to different rules (steps in the workflow). There is an example for Case Western Reserve University SLURM cluster . See also the SLURM documentation and the SLURM Snakemake profile documentation for information on how to write this file. You can see the names of the steps in the workflow at any time by running: $ poetry run snakemake --list So for example you might: Download the example cluster configuration. $ wget https://github.com/frankier/skelshop/blob/master/contrib/slurm/skels.tracked.clusc.json Edit it if need be. Then run the following command after editing the placeholders (at least NUM_JOBS , SING_EXTRA_ARGS , VIDEO_BASE and DUMP_BASE : $ SIF_PATH=$(pwd)/skelshop.sif \\ SNAKEFILE=/opt/skelshop/workflow/Snakefile \\ CLUSC_CONF=$(pwd)/skels.tracked.clusc.json \\ NUM_JOBS=42 \\ SING_EXTRA_ARGS=\"--bind /path/to/my/extra/bind\" \\ ~/singslurm2/run.sh \\ tracked_all \\ --config \\ VIDEO_BASE=/path/to/my/video/corpus/ \\ DUMP_BASE=/path/to/my/dump/directory Please see the singslurm repository for more information about the environment variables passed to singslurm2/run.sh .","title":"Running Snakemake on a SLURM cluster"},{"location":"snakemake/#integrating-skelshop-into-your-own-pipelines","text":"In case you are using SkelShop as part of a larger pipeline or want to further customise your workflow, you should write your own Snakefile. See the Snakemake documentation . You may like to use the rules and scripts from SkelShop. In this case the current best approach is to copy or symlink everything under workflow/rules and workflow/scripts into your own workflow directory.","title":"Integrating SkelShop into your own pipelines"},{"location":"snakemake/#other-hpc-utilities","text":"There are some examples of how to run which are specific to the Case Western Reserve University SLURM cluster in the contrib/slurm directory .","title":"Other HPC utilities"},{"location":"tracking/","text":"The tracker is a stage that each frame takes an untracked bundle of poses and gives them IDs. skelshop.bbtrack.TrackStage # A pipeline stage wrapping the skelshop.track.PoseTrack generic approach to distance-based tracking. Internally it uses the following class, which implements a generic approach to online pose tracking: skelshop.track.track.PoseTrack # Performs generic distance-based pose tracking. __init__ ( self , spec ) special # Takes a TrackingSpec and constructs the corresponding tracking algorithm. Source code in skelshop/track/track.py def __init__ ( self , spec : TrackingSpec ): \"\"\" Takes a TrackingSpec and constructs the corresponding tracking algorithm. \"\"\" self . next_id = 0 self . prev_tracked : FrameBuf = ( collections . deque ( maxlen = spec . prev_frame_buf_size ) ) self . spec = spec Which is in turn configured using a: skelshop.track.spec.TrackingSpec dataclass # A domain specific language for threshold distance-based style tracking. Candidate poses are first filtered using cand_filter and then procedure is executed to decide how to assign the candidates. Several configurations are given in: skelshop.track.confs.CONFS # A dictionary of ready-made tracking specs Attributes: Name Type Description \"lighttrackish\" TrackingSpec A LightTrack-like algoirhtm using greedy matching \"opt_lighttrack\" TrackingSpec A LightTrack-like algorithm which attempts to make optimal matches \"deepsortlike\" TrackingSpec A DeepSort-like algorithm (currently missing Kalman filtering) There are references to two systems in the name: LightTrack 1 and DeepSort 2 . The configurations are inspired by these, but not exact implementations. Looking at the implementations in skelshop.track.confs.CONFS is a good starting point for adding new configurations, or extending the tracking, e.g. with a new approach to reidentification. Guanghan Ning, Heng Huang (2019) LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking https://arxiv.org/abs/1905.02822 \u21a9 Nicolai Wojke, Alex Bewley, Dietrich Paulus (2017) Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 \u21a9","title":"Tracking"},{"location":"tracking/#skelshop.bbtrack.TrackStage","text":"A pipeline stage wrapping the skelshop.track.PoseTrack generic approach to distance-based tracking. Internally it uses the following class, which implements a generic approach to online pose tracking:","title":"TrackStage"},{"location":"tracking/#skelshop.track.track.PoseTrack","text":"Performs generic distance-based pose tracking.","title":"PoseTrack"},{"location":"tracking/#skelshop.track.track.PoseTrack.__init__","text":"Takes a TrackingSpec and constructs the corresponding tracking algorithm. Source code in skelshop/track/track.py def __init__ ( self , spec : TrackingSpec ): \"\"\" Takes a TrackingSpec and constructs the corresponding tracking algorithm. \"\"\" self . next_id = 0 self . prev_tracked : FrameBuf = ( collections . deque ( maxlen = spec . prev_frame_buf_size ) ) self . spec = spec Which is in turn configured using a:","title":"__init__()"},{"location":"tracking/#skelshop.track.spec.TrackingSpec","text":"A domain specific language for threshold distance-based style tracking. Candidate poses are first filtered using cand_filter and then procedure is executed to decide how to assign the candidates. Several configurations are given in:","title":"TrackingSpec"},{"location":"tracking/#skelshop.track.confs.CONFS","text":"A dictionary of ready-made tracking specs Attributes: Name Type Description \"lighttrackish\" TrackingSpec A LightTrack-like algoirhtm using greedy matching \"opt_lighttrack\" TrackingSpec A LightTrack-like algorithm which attempts to make optimal matches \"deepsortlike\" TrackingSpec A DeepSort-like algorithm (currently missing Kalman filtering) There are references to two systems in the name: LightTrack 1 and DeepSort 2 . The configurations are inspired by these, but not exact implementations. Looking at the implementations in skelshop.track.confs.CONFS is a good starting point for adding new configurations, or extending the tracking, e.g. with a new approach to reidentification. Guanghan Ning, Heng Huang (2019) LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking https://arxiv.org/abs/1905.02822 \u21a9 Nicolai Wojke, Alex Bewley, Dietrich Paulus (2017) Simple Online and Realtime Tracking with a Deep Association Metric https://arxiv.org/abs/1703.07402 \u21a9","title":"CONFS"},{"location":"usage-examples/","text":"Usage examples # Here are some usage examples. There is more help available through the `--help' flag and on the CLI reference page . Dumping/tracking # A working OpenPose install is required for dumping (and only for dumping). We could dump some untracked skeletons with a version of OpenPose we have compiled ourselves like so: LD_LIBRARY_PATH=$OPENPOSE/build/src/openpose/ \\ PYTHONPATH=$OPENPOSE/build/python/ \\ MODEL_FOLDER=$OPENPOSE/models \\ poetry run python -m skelshop \\ dump \\ --mode BODY_25 \\ video_in.mp4 pose_data.h5 We could also use the Singularity or Docker. OpenPose is installed in the image and everything is setup up for us so we can just run: $ singularity pull skelshop.sif docker://frankierr/skelshop:focal_nvcaffe $ singularity exec --nv skelshop.sif python /opt/skelshop/skelshop dump video_in.mp4 pose_data.h5 OR $ docker run --nv frankierr/skelshop:focal_nvcaffe python /opt/skelshop/skelshop dump video_in.mp4 pose_data.h5 You can track an existing dump using the filter command with the --track flag or apply tracking at the same time as dumping. Currently scene segmentation is expected in this case, which can be done using CSV dumps generated with PySceneDetect's CLI . For more information see the Snakefile , and the help provided with python skelshop --help . Conversion # Convert from a zip file containing files named like so: XXXXXXX_0000000NNNNN_keypoints.json $ poetry run python skelshop conv Convert from a tar file containing similarly name files in order : $ poetry run python skelshop conv Drawing/playing # Play a video with sticks superimposed (without sound): $ poetry run python skelshop playsticks pose_data.h5 video_in.mp4 video_out.mp4 Press h/? to see the keyboard controls available in the player. Dump a video with sticks superimposed (without sound): $ poetry run python skelshop drawsticks pose_data.h5 video_in.mp4 video_out.mp4","title":"Misc. usage examples"},{"location":"usage-examples/#usage-examples","text":"Here are some usage examples. There is more help available through the `--help' flag and on the CLI reference page .","title":"Usage examples"},{"location":"usage-examples/#dumpingtracking","text":"A working OpenPose install is required for dumping (and only for dumping). We could dump some untracked skeletons with a version of OpenPose we have compiled ourselves like so: LD_LIBRARY_PATH=$OPENPOSE/build/src/openpose/ \\ PYTHONPATH=$OPENPOSE/build/python/ \\ MODEL_FOLDER=$OPENPOSE/models \\ poetry run python -m skelshop \\ dump \\ --mode BODY_25 \\ video_in.mp4 pose_data.h5 We could also use the Singularity or Docker. OpenPose is installed in the image and everything is setup up for us so we can just run: $ singularity pull skelshop.sif docker://frankierr/skelshop:focal_nvcaffe $ singularity exec --nv skelshop.sif python /opt/skelshop/skelshop dump video_in.mp4 pose_data.h5 OR $ docker run --nv frankierr/skelshop:focal_nvcaffe python /opt/skelshop/skelshop dump video_in.mp4 pose_data.h5 You can track an existing dump using the filter command with the --track flag or apply tracking at the same time as dumping. Currently scene segmentation is expected in this case, which can be done using CSV dumps generated with PySceneDetect's CLI . For more information see the Snakefile , and the help provided with python skelshop --help .","title":"Dumping/tracking"},{"location":"usage-examples/#conversion","text":"Convert from a zip file containing files named like so: XXXXXXX_0000000NNNNN_keypoints.json $ poetry run python skelshop conv Convert from a tar file containing similarly name files in order : $ poetry run python skelshop conv","title":"Conversion"},{"location":"usage-examples/#drawingplaying","text":"Play a video with sticks superimposed (without sound): $ poetry run python skelshop playsticks pose_data.h5 video_in.mp4 video_out.mp4 Press h/? to see the keyboard controls available in the player. Dump a video with sticks superimposed (without sound): $ poetry run python skelshop drawsticks pose_data.h5 video_in.mp4 video_out.mp4","title":"Drawing/playing"},{"location":"user-guide-intro/","text":"User guide introduction # This user guide is written bottom-up, with later sections detailing conveniences, in particular the usage of Snakemake , that it is likely you will want to make use of in most cases. For this reason it is recommended to at least skim the all parts of the guide before proceeding with using SkelShop.","title":"Introduction"},{"location":"user-guide-intro/#user-guide-introduction","text":"This user guide is written bottom-up, with later sections detailing conveniences, in particular the usage of Snakemake , that it is likely you will want to make use of in most cases. For this reason it is recommended to at least skim the all parts of the guide before proceeding with using SkelShop.","title":"User guide introduction"}]}